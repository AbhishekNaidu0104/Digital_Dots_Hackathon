# -*- coding: utf-8 -*-
"""Untitled45.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EuHqxrRl4fRZbb_0m9Qaxj4Q35zZ10hU
"""

# 📦 Install dependencies (run only once)
!pip install git+https://github.com/openai/CLIP.git
!pip install openai-whisper moviepy opencv-python Pillow regex tqdm ffmpeg-python

# 📚 Import libraries
import os
import cv2
import torch
import clip
import whisper
from PIL import Image
from tqdm import tqdm
from moviepy.editor import VideoFileClip
from IPython.display import display, Image as IPyImage
from google.colab import files
import random

# 📤 Upload video
uploaded = files.upload()
video_path = list(uploaded.keys())[0]

# ⚙ Load models
device = "cuda" if torch.cuda.is_available() else "cpu"
clip_model, preprocess = clip.load("ViT-B/32", device=device)
whisper_model = whisper.load_model("base")

# 🎞 Extract frames (1 per second)
frames_dir = "frames"
os.makedirs(frames_dir, exist_ok=True)

cap = cv2.VideoCapture(video_path)
fps = int(cap.get(cv2.CAP_PROP_FPS))
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
duration = int(total_frames / fps)

print(f"🎥 Video duration: {duration}s, FPS: {fps}, Total frames: {total_frames}")

frame_interval = fps
frame_number = 0
saved = 0

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    if frame_number % frame_interval == 0:
        frame_path = os.path.join(frames_dir, f"frame_{frame_number:05d}.jpg")
        cv2.imwrite(frame_path, frame)
        saved += 1
    frame_number += 1

cap.release()
print(f"✅ Extracted {saved} frames (1 per second)")

# 💬 CLIP prompts
prompts = [
    "a safe scene",
    "nudity",
    "pornographic content",
    "a person showing a middle finger",
    "a rude gesture",
    "explicit content",
    "kissing scene",
    "sexual feeling",
    "graphic violence"
]
text_tokens = clip.tokenize(prompts).to(device)
safe_label = "a safe scene"
unsafe_labels = prompts[1:]
threshold = 0.5

# 🔍 Analyze frames using CLIP
print("\n🔍 Scanning Frames with CLIP...")

flagged_frames = []
safe_frames = []

for frame_file in tqdm(sorted(os.listdir(frames_dir))):
    frame_path = os.path.join(frames_dir, frame_file)
    image = preprocess(Image.open(frame_path)).unsqueeze(0).to(device)

    with torch.no_grad():
        logits_per_image, _ = clip_model(image, text_tokens)
        probs = logits_per_image.softmax(dim=-1).cpu().numpy()[0]

    label_confidences = dict(zip(prompts, probs))

    detected_issues = []
    for label in unsafe_labels:
        if label_confidences[label] > threshold:
            detected_issues.append((label, label_confidences[label]))

    if detected_issues:
        detected_issues.sort(key=lambda x: x[1], reverse=True)
        top_issue = detected_issues[0]
        flagged_frames.append({
            "path": frame_path,
            "issues": detected_issues,
            "top_reason": top_issue[0],
            "top_confidence": top_issue[1]
        })
    else:
        safe_frames.append({
            "path": frame_path,
            "reason": safe_label,
            "confidence": label_confidences[safe_label]
        })

# 🔈 Transcribe audio with Whisper
print("\n🔈 Extracting & Transcribing Audio...")
audio_path = "audio.wav"
video_clip = VideoFileClip(video_path)
video_clip.audio.write_audiofile(audio_path, codec='pcm_s16le', verbose=False, logger=None)

result = whisper_model.transcribe(audio_path)
transcript = result["text"]

print("\n📝 Transcript:")
print(transcript)

# 🚫 Flag vulgar language
vulgar_words = [
    "fuck", "shit", "bitch", "asshole", "nude", "naked", "porn", "violence",
    "kill", "rape", "gun", "murder", "suck", "dick", "blowjob", "slut"
]
flagged_words = [w for w in vulgar_words if w in transcript.lower()]

# ✅ Final Decision
print("\n==========================")
if flagged_frames or flagged_words:
    print("🚫 VIDEO FLAGGED AS: UNSAFE")

    if flagged_frames:
        print(f"\n📸 Unsafe Frames Detected ({len(flagged_frames)} total):")
        max_to_show = 5

        for i, frame in enumerate(sorted(flagged_frames, key=lambda x: x["top_confidence"], reverse=True)[:max_to_show]):
            print(f"\n🖼 Frame {i+1}:")
            print(f"⚠ Top Issue: {frame['top_reason']} (Confidence: {frame['top_confidence']:.2f})")
            print("⚠ All Detected Issues:")
            for label, conf in frame["issues"]:
                print(f"   - {label} ({conf:.2f})")
            display(IPyImage(filename=frame["path"]))

        if len(flagged_frames) > max_to_show:
            print(f"...and {len(flagged_frames) - max_to_show} more unsafe frames detected.")

    if flagged_words:
        print(f"\n🔊 Unsafe Words Detected in Audio:")
        for word in flagged_words:
            print(f"⚠ {word}")
else:
    print("✅ VIDEO FLAGGED AS: SAFE")
    safe_frame = random.choice(safe_frames)
    print(f"\n📸 Safe Frame Example:")
    print(f"🟢 Action: {safe_frame['reason']} (Confidence: {safe_frame['confidence']:.2f})")
    display(IPyImage(filename=safe_frame["path"]))
print("==========================")